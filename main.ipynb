{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np \n",
    "# import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to generate a smooth plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_window_filter(data, window_size):\n",
    "    if window_size % 2 == 0:\n",
    "        window_size += 1\n",
    "    pad = window_size // 2\n",
    "    padded_data = np.pad(data, (pad, pad), mode='edge')\n",
    "    smoothed_data = np.zeros_like(data)\n",
    "    for i in range(len(data)):\n",
    "        smoothed_data[i] = np.mean(padded_data[i:i + window_size])\n",
    "    return smoothed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(non optimized) function to generate bin indices for discretization of continuous state variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_which_bin(state, state_space):\n",
    "    indices = []\n",
    "    disc_state = []\n",
    "    st = state[0] if type(state) == type(()) else state  # because env.reset returns a tuple but env.step returns observations ... ABSOLUTELY WEIRD\n",
    "    for idx, obs in enumerate(st): \n",
    "        for jdx, bin_low in enumerate(state_space[idx]): \n",
    "            if obs < bin_low:\n",
    "                indices.append(jdx)\n",
    "                disc_state.append(bin_low)\n",
    "                break\n",
    "        else:\n",
    "            indices.append(state_space[idx].shape[0]-1)\n",
    "            disc_state.append(state_space[idx][-1])\n",
    "    return disc_state, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to implement Q-learning\n",
    "\n",
    "1. select action with epsilon-greedy \n",
    "2. perform the action on the environment and receive new state and reward\n",
    "3. update Q table with Bellman's equation\n",
    "\n",
    "Note: epsilon is decaying as the agent trains, so exploit is being preferred as episodes go on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(env, Q_table, state_space, EPISODES, EPSILON, LEARNING_RATE, DISCOUNT_FACTOR, verbose=True):\n",
    "    all_rewards = []\n",
    "    for episode in range(EPISODES):\n",
    "        episode_reward = 0\n",
    "        state = env.reset()\n",
    "        _, state_indices = show_which_bin(state, state_space)\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            if np.random.rand() < EPSILON:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "            else:  \n",
    "                action = np.argmax(Q_table[:, state_indices[0], state_indices[1], state_indices[2], state_indices[3]]) \n",
    "\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            _, next_state_indices = show_which_bin(new_state, state_space) \n",
    "            terminated = terminated or truncated\n",
    "\n",
    "            new_Q = LEARNING_RATE * (reward + DISCOUNT_FACTOR * (np.max(Q_table[:, next_state_indices[0], next_state_indices[1], next_state_indices[2], next_state_indices[3]]) - Q_table[action, state_indices[0], state_indices[1], state_indices[2], state_indices[3]]))\n",
    "            Q_table[action, state_indices[0], state_indices[1], state_indices[2], state_indices[3]] += new_Q\n",
    "            \n",
    "            if terminated:\n",
    "                break\n",
    "\n",
    "            state = new_state\n",
    "            state_indices = next_state_indices\n",
    "            episode_reward += reward\n",
    "        if verbose:\n",
    "            print(\"episode:\", episode, '\\treward:', episode_reward, '\\tepsilon:', EPSILON) \n",
    "        EPSILON *= 0.9999\n",
    "        all_rewards.append(episode_reward)\n",
    "    return Q_table, all_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space.n)\n",
    "print(env.observation_space.shape)  # [low values, high values], shape, type\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create (discrete) state space and zero Q_table (with all possible states and coupled actions) \n",
    "\n",
    "The Q-table holds the Q-values for each state-action pair. For continuous state spaces like CartPole, the state space is discretized into bins (state space discretization). This makes it possible to represent the Q-values in a table. The actions are the two possible movements (left or right).\n",
    "For CartPole, you could discretize the state space into grids for each of the four state variables (position, velocity, angle, angular velocity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 20\n",
    "\n",
    "state_space = np.array([\n",
    "    np.linspace(env.observation_space.low[0], env.observation_space.high[0], n_bins),   # cart position\n",
    "    np.linspace(-5, 5, n_bins),                                                         # cart velocity\n",
    "    np.linspace(env.observation_space.low[2], env.observation_space.high[2], n_bins),   # pole angle (rad)\n",
    "    np.linspace(-1, 1, n_bins)                                                          # pole angular velocity\n",
    "])  \n",
    "Q_table = np.zeros([2] + state_space.shape[0] * [state_space.shape[1]])\n",
    "Q_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.95\n",
    "EPSILON = 0.5\n",
    "\n",
    "EPISODES = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_Q_table, final_results = learn(env, Q_table, state_space, EPISODES, EPSILON, LEARNING_RATE, DISCOUNT_FACTOR, verbose=True)\n",
    "final_results_smooth = (average_window_filter(final_results, window_size=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(final_results)\n",
    "plt.plot(final_results_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the agent and save the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes_and_create_video(env, Q_table, state_space, num_episodes, output_file, fps=30):\n",
    "    frames = []  \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        _, state_indices = show_which_bin(state, state_space)\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "            action = np.argmax(Q_table[:, state_indices[0], state_indices[1], state_indices[2], state_indices[3]])\n",
    "            new_state, _, terminated, truncated, _ = env.step(action)\n",
    "            terminated = terminated or truncated\n",
    "            _, state_indices = show_which_bin(new_state, state_space)\n",
    "    env.close()  \n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, (width, height))\n",
    "    for frame in frames:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame)\n",
    "    out.release()  \n",
    "    print(f\"Video saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate two videos for the trained agent and a random one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")  \n",
    "\n",
    "run_episodes_and_create_video(env, filled_Q_table, state_space, num_episodes=10, output_file=\"cartpole_output.mp4\")\n",
    "\n",
    "random_Q_table = np.random.random(([2] + state_space.shape[0] * [state_space.shape[1]])) \n",
    "run_episodes_and_create_video(env, random_Q_table, state_space, num_episodes=250, output_file=\"cartpole_output_random.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
